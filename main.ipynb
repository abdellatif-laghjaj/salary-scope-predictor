{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7694b9f",
   "metadata": {},
   "source": [
    "# Developer Salary Prediction Model\n",
    "\n",
    "This notebook builds a machine learning model to predict developer salaries based on Stack Overflow survey data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Data Cleaning and Preprocessing](#data-cleaning)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Training and Evaluation](#model-training)\n",
    "6. [Model Optimization](#model-optimization)\n",
    "7. [Final Model Persistence](#model-persistence)\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095860be",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration {#data-loading}\n",
    "\n",
    "Loading the Stack Overflow Developer Survey data and performing initial exploration.\n",
    "You can download data from here: https://survey.stackoverflow.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/survey_results_public.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns in dataset: {len(df.columns)}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ecd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652b33a",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing {#data-cleaning}\n",
    "\n",
    "### 2.1 Feature Selection and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for salary prediction\n",
    "relevant_columns = [\n",
    "    \"Country\", \n",
    "    \"EdLevel\", \n",
    "    \"YearsCodePro\", \n",
    "    \"Employment\", \n",
    "    \"ConvertedCompYearly\",\n",
    "    \"Age\",\n",
    "    \"OrgSize\",\n",
    "    \"DevType\"\n",
    "]\n",
    "\n",
    "# Filter columns that exist in the dataset\n",
    "available_columns = [col for col in relevant_columns if col in df.columns]\n",
    "df_selected = df[available_columns].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "df_selected = df_selected.rename({\"ConvertedCompYearly\": \"Salary\"}, axis=1)\n",
    "\n",
    "print(f\"Selected features: {list(df_selected.columns)}\")\n",
    "print(f\"\\nDataset shape after selection: {df_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_quality_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"=== Data Quality Assessment ===\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_data = df_selected.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df_selected)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "})\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df_selected.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employment_filter",
   "metadata": {},
   "source": [
    "### 2.2 Employment Status Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f71caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for valid salary data and full-time employment\n",
    "print(\"Employment status distribution:\")\n",
    "print(df_selected['Employment'].value_counts())\n",
    "\n",
    "# Keep only full-time employed individuals with salary data\n",
    "df_clean = df_selected[\n",
    "    (df_selected[\"Salary\"].notnull()) & \n",
    "    (df_selected[\"Employment\"] == \"Employed, full-time\")\n",
    "].copy()\n",
    "\n",
    "# Drop employment column as it's no longer needed\n",
    "df_clean = df_clean.drop(\"Employment\", axis=1)\n",
    "\n",
    "print(f\"\\nRows after employment filtering: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "salary_cleaning",
   "metadata": {},
   "source": [
    "### 2.3 Salary Data Cleaning and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "salary_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary distribution analysis\n",
    "print(\"=== Salary Distribution Analysis ===\")\n",
    "print(df_clean['Salary'].describe())\n",
    "\n",
    "# Visualize salary distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_clean['Salary'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Salary Distribution')\n",
    "axes[0].set_xlabel('Salary (USD)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_clean['Salary'])\n",
    "axes[1].set_title('Salary Box Plot')\n",
    "axes[1].set_ylabel('Salary (USD)')\n",
    "\n",
    "# Log scale histogram\n",
    "axes[2].hist(np.log10(df_clean['Salary']), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Log10 Salary Distribution')\n",
    "axes[2].set_xlabel('Log10(Salary)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outlier_removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove salary outliers using IQR method with domain knowledge\n",
    "def remove_salary_outliers(df, column='Salary'):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Use broader range for salary data + domain knowledge\n",
    "    lower_bound = max(Q1 - 2.5 * IQR, 10000)  # Minimum realistic salary\n",
    "    upper_bound = min(Q3 + 2.5 * IQR, 500000)  # Maximum realistic salary\n",
    "    \n",
    "    print(f\"Salary range: ${lower_bound:,.0f} - ${upper_bound:,.0f}\")\n",
    "    \n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "df_clean = remove_salary_outliers(df_clean)\n",
    "print(f\"Rows after salary outlier removal: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "country_cleaning",
   "metadata": {},
   "source": [
    "### 2.4 Country Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d715eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze country distribution\n",
    "print(\"Top 20 countries by respondent count:\")\n",
    "country_counts = df_clean['Country'].value_counts()\n",
    "print(country_counts.head(20))\n",
    "\n",
    "# Visualize country distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "country_counts.head(15).plot(kind='bar')\n",
    "plt.title('Top 15 Countries by Survey Respondents')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Respondents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_categories(categories, cutoff):\n",
    "    \"\"\"Group low-frequency categories into 'Other' to reduce noise\"\"\"\n",
    "    categorical_map = {}\n",
    "    for category, count in categories.items():\n",
    "        if count >= cutoff:\n",
    "            categorical_map[category] = category\n",
    "        else:\n",
    "            categorical_map[category] = \"Other\"\n",
    "    return categorical_map\n",
    "\n",
    "# Apply country grouping with higher threshold for better model performance\n",
    "country_map = shorten_categories(df_clean['Country'].value_counts(), 300)\n",
    "df_clean['Country'] = df_clean['Country'].map(country_map)\n",
    "\n",
    "print(\"Countries after grouping:\")\n",
    "print(df_clean['Country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis {#eda}\n",
    "\n",
    "### 3.1 Salary by Country Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Other' category for cleaner analysis\n",
    "df_clean = df_clean[df_clean['Country'] != 'Other']\n",
    "\n",
    "# Create enhanced salary by country visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "# Box plot\n",
    "df_clean.boxplot(column=\"Salary\", by=\"Country\", ax=axes[0])\n",
    "axes[0].set_title(\"Salary Distribution by Country\")\n",
    "axes[0].set_ylabel(\"Salary (USD)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mean salary by country\n",
    "mean_salaries = df_clean.groupby('Country')['Salary'].mean().sort_values(ascending=False)\n",
    "mean_salaries.plot(kind='bar', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title(\"Average Salary by Country\")\n",
    "axes[1].set_ylabel(\"Average Salary (USD)\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage salaries by country:\")\n",
    "print(mean_salaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering {#feature-engineering}\n",
    "\n",
    "### 4.1 Years of Experience Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0861c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values first\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Remove rows with missing critical features\n",
    "df_clean = df_clean.dropna(subset=['Country', 'EdLevel', 'YearsCodePro'])\n",
    "\n",
    "print(f\"\\nRows after removing missing values: {len(df_clean)}\")\n",
    "print(\"\\nUnique values in YearsCodePro:\")\n",
    "print(df_clean['YearsCodePro'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_experience(x):\n",
    "    \"\"\"Convert experience categories to numeric values\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if \"More than 50 years\" in str(x):\n",
    "        return 50\n",
    "    if \"Less than 1 year\" in str(x):\n",
    "        return 0.5\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_clean[\"YearsCodePro\"] = df_clean[\"YearsCodePro\"].apply(clean_experience)\n",
    "\n",
    "# Remove any remaining NaN values in experience\n",
    "df_clean = df_clean.dropna(subset=['YearsCodePro'])\n",
    "\n",
    "print(f\"Experience range: {df_clean['YearsCodePro'].min()} - {df_clean['YearsCodePro'].max()}\")\n",
    "print(f\"\\nExperience distribution:\")\n",
    "print(df_clean['YearsCodePro'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "education_processing",
   "metadata": {},
   "source": [
    "### 4.2 Education Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Education level distribution:\")\n",
    "print(df_clean['EdLevel'].value_counts())\n",
    "\n",
    "# Visualize education distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_clean['EdLevel'].value_counts().plot(kind='bar')\n",
    "plt.title('Education Level Distribution')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced education mapping with better categorization\n",
    "def clean_education(x):\n",
    "    \"\"\"Standardize education levels into meaningful categories\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    x = str(x).lower()\n",
    "    \n",
    "    if \"bachelor\" in x:\n",
    "        return \"Bachelor's degree\"\n",
    "    elif \"master\" in x:\n",
    "        return \"Master's degree\"\n",
    "    elif \"doctoral\" in x or \"phd\" in x:\n",
    "        return \"Doctoral degree\"\n",
    "    elif \"professional\" in x or \"law\" in x or \"medicine\" in x:\n",
    "        return \"Professional degree\"\n",
    "    elif \"associate\" in x:\n",
    "        return \"Associate degree\"\n",
    "    elif \"high school\" in x or \"secondary\" in x:\n",
    "        return \"High school\"\n",
    "    else:\n",
    "        return \"Other/Some college\"\n",
    "\n",
    "df_clean[\"EdLevel\"] = df_clean[\"EdLevel\"].apply(clean_education)\n",
    "\n",
    "print(\"Education levels after cleaning:\")\n",
    "print(df_clean[\"EdLevel\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding",
   "metadata": {},
   "source": [
    "### 4.3 Feature Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store label encoders for later use\n",
    "le_education = LabelEncoder()\n",
    "le_country = LabelEncoder()\n",
    "\n",
    "# Fit and transform categorical variables\n",
    "df_encoded = df_clean.copy()\n",
    "df_encoded['EdLevel_encoded'] = le_education.fit_transform(df_encoded['EdLevel'])\n",
    "df_encoded['Country_encoded'] = le_country.fit_transform(df_encoded['Country'])\n",
    "\n",
    "print(\"Label encoding mappings:\")\n",
    "print(\"\\nEducation:\")\n",
    "for i, level in enumerate(le_education.classes_):\n",
    "    print(f\"{i}: {level}\")\n",
    "    \n",
    "print(\"\\nCountry:\")\n",
    "for i, country in enumerate(le_country.classes_):\n",
    "    print(f\"{i}: {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_columns = ['Country_encoded', 'EdLevel_encoded', 'YearsCodePro']\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['Salary']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation {#model-training}\n",
    "\n",
    "### 5.1 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_splitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=X['Country_encoded']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Training set percentage: {X_train.shape[0] / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_comparison",
   "metadata": {},
   "source": [
    "### 5.2 Model Comparison and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'CV_RMSE': cv_rmse,\n",
    "        'Overfitting': train_rmse - test_rmse\n",
    "    }\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    result, trained_model = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    results.append(result)\n",
    "    trained_models[name] = trained_model\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(2)\n",
    "print(\"\\n=== Model Comparison Results ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-optimization",
   "metadata": {},
   "source": [
    "## 6. Model Optimization {#model-optimization}\n",
    "\n",
    "### 6.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model for optimization\n",
    "best_model_name = results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest (typically performs well)\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {np.sqrt(-rf_grid.best_score_):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimized model\n",
    "optimized_model = rf_grid.best_estimator_\n",
    "opt_results, _ = evaluate_model(optimized_model, X_train, X_test, y_train, y_test, 'Optimized Random Forest')\n",
    "\n",
    "print(\"\\n=== Optimized Model Performance ===\")\n",
    "for key, value in opt_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "### 6.2 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': optimized_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance - Optimized Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_validation",
   "metadata": {},
   "source": [
    "### 6.3 Model Validation and Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "y_pred = optimized_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Salary')\n",
    "axes[0, 0].set_ylabel('Predicted Salary')\n",
    "axes[0, 0].set_title('Predicted vs Actual Salary')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0, 1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Salary')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1, 0].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residuals Distribution')\n",
    "\n",
    "# QQ plot for residuals\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional validation metrics\n",
    "print(f\"\\nModel Validation Metrics:\")\n",
    "print(f\"Mean Absolute Percentage Error: {np.mean(np.abs(residuals) / y_test) * 100:.2f}%\")\n",
    "print(f\"Median Absolute Error: {np.median(np.abs(residuals)):.2f}\")\n",
    "print(f\"90th Percentile Error: {np.percentile(np.abs(residuals), 90):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-persistence",
   "metadata": {},
   "source": [
    "## 7. Final Model Persistence and Deployment Preparation {#model-persistence}\n",
    "\n",
    "### 7.1 Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_persistence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive model package\n",
    "model_package = {\n",
    "    \"model\": optimized_model,\n",
    "    \"le_country\": le_country,\n",
    "    \"le_education\": le_education,\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"model_performance\": opt_results,\n",
    "    \"feature_importance\": feature_importance.to_dict(),\n",
    "    \"training_data_stats\": {\n",
    "        \"n_samples\": len(X_train),\n",
    "        \"salary_mean\": float(y_train.mean()),\n",
    "        \"salary_std\": float(y_train.std()),\n",
    "        \"countries\": list(le_country.classes_),\n",
    "        \"education_levels\": list(le_education.classes_)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model package\n",
    "with open(\"salary_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_package, file)\n",
    "\n",
    "print(\"Model package saved successfully!\")\n",
    "print(f\"\\nModel package contents:\")\n",
    "for key in model_package.keys():\n",
    "    print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction_interface",
   "metadata": {},
   "source": [
    "### 7.2 Prediction Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_salary(country, education, years_experience, model_package):\n",
    "    \"\"\"\n",
    "    Predict salary based on input features\n",
    "    \n",
    "    Args:\n",
    "        country (str): Country name\n",
    "        education (str): Education level\n",
    "        years_experience (float): Years of coding experience\n",
    "        model_package (dict): Loaded model package\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results with confidence interval\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract components\n",
    "        model = model_package['model']\n",
    "        le_country = model_package['le_country']\n",
    "        le_education = model_package['le_education']\n",
    "        \n",
    "        # Prepare input\n",
    "        country_encoded = le_country.transform([country])[0]\n",
    "        education_encoded = le_education.transform([education])[0]\n",
    "        \n",
    "        # Create feature vector\n",
    "        X_input = np.array([[country_encoded, education_encoded, years_experience]])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(X_input)[0]\n",
    "        \n",
    "        # Calculate prediction interval using ensemble predictions\n",
    "        if hasattr(model, 'estimators_'):\n",
    "            tree_predictions = [tree.predict(X_input)[0] for tree in model.estimators_]\n",
    "            prediction_std = np.std(tree_predictions)\n",
    "            confidence_interval = {\n",
    "                'lower': prediction - 1.96 * prediction_std,\n",
    "                'upper': prediction + 1.96 * prediction_std\n",
    "            }\n",
    "        else:\n",
    "            confidence_interval = {'lower': None, 'upper': None}\n",
    "        \n",
    "        return {\n",
    "            'predicted_salary': round(prediction, 2),\n",
    "            'confidence_interval': confidence_interval,\n",
    "            'input_features': {\n",
    "                'country': country,\n",
    "                'education': education,\n",
    "                'years_experience': years_experience\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test the prediction function\n",
    "test_predictions = [\n",
    "    (\"United States of America\", \"Master's degree\", 15),\n",
    "    (\"Germany\", \"Bachelor's degree\", 5),\n",
    "    (\"India\", \"Master's degree\", 10),\n",
    "    (\"United Kingdom of Great Britain and Northern Ireland\", \"Doctoral degree\", 20)\n",
    "]\n",
    "\n",
    "print(\"=== Sample Predictions ===\")\n",
    "for country, education, experience in test_predictions:\n",
    "    result = predict_salary(country, education, experience, model_package)\n",
    "    if 'error' not in result:\n",
    "        print(f\"\\n{country}, {education}, {experience} years:\")\n",
    "        print(f\"  Predicted Salary: ${result['predicted_salary']:,.2f}\")\n",
    "        if result['confidence_interval']['lower']:\n",
    "            print(f\"  95% CI: ${result['confidence_interval']['lower']:,.2f} - ${result['confidence_interval']['upper']:,.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nError predicting for {country}, {education}, {experience} years: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_validation_final",
   "metadata": {},
   "source": [
    "### 7.3 Model Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the saved model\n",
    "with open(\"salary_model.pkl\", \"rb\") as file:\n",
    "    loaded_model_package = pickle.load(file)\n",
    "\n",
    "# Verify model loading\n",
    "loaded_model = loaded_model_package['model']\n",
    "loaded_le_country = loaded_model_package['le_country']\n",
    "loaded_le_education = loaded_model_package['le_education']\n",
    "\n",
    "# Test predictions with loaded model\n",
    "test_X = X_test.iloc[:5]  # Test with first 5 samples\n",
    "test_y = y_test.iloc[:5]\n",
    "\n",
    "loaded_predictions = loaded_model.predict(test_X)\n",
    "original_predictions = optimized_model.predict(test_X)\n",
    "\n",
    "print(\"Model Loading Verification:\")\n",
    "print(f\"Predictions match: {np.allclose(loaded_predictions, original_predictions)}\")\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\n=== Final Model Summary ===\")\n",
    "print(f\"Model Type: {type(loaded_model).__name__}\")\n",
    "print(f\"Test RMSE: ${loaded_model_package['model_performance']['Test_RMSE']:,.2f}\")\n",
    "print(f\"Test R²: {loaded_model_package['model_performance']['Test_R2']:.3f}\")\n",
    "print(f\"Test MAE: ${loaded_model_package['model_performance']['Test_MAE']:,.2f}\")\n",
    "print(f\"Training Samples: {loaded_model_package['training_data_stats']['n_samples']:,}\")\n",
    "print(f\"Supported Countries: {len(loaded_model_package['training_data_stats']['countries'])}\")\n",
    "print(f\"Education Levels: {len(loaded_model_package['training_data_stats']['education_levels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment_notes",
   "metadata": {},
   "source": [
    "## 8. Deployment Notes and Recommendations\n",
    "\n",
    "### Model Performance Summary\n",
    "- **Model Type**: Optimized Random Forest Regressor\n",
    "- **Key Features**: Country, Education Level, Years of Experience\n",
    "- **Performance**: Test RMSE and R² scores indicate good predictive capability\n",
    "- **Validation**: Cross-validation and residual analysis confirm model reliability\n",
    "\n",
    "### Production Considerations\n",
    "1. **Input Validation**: Ensure all inputs are validated against known categories\n",
    "2. **Model Monitoring**: Track prediction accuracy over time\n",
    "3. **Data Drift**: Monitor for changes in salary distributions\n",
    "4. **Model Updates**: Retrain periodically with new survey data\n",
    "5. **Error Handling**: Implement robust error handling for unknown inputs\n",
    "\n",
    "### Usage Instructions\n",
    "```python\n",
    "# Load the model\n",
    "with open('salary_model.pkl', 'rb') as f:\n",
    "    model_package = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "result = predict_salary('United States of America', 'Master\\'s degree', 10, model_package)\n",
    "print(f\"Predicted salary: ${result['predicted_salary']:,.2f}\")\n",
    "```\n",
    "\n",
    "### Model Limitations\n",
    "- Predictions are based on Stack Overflow survey data\n",
    "- Limited to countries and education levels in training data\n",
    "- May not reflect real-time market conditions\n",
    "- Results should be used as estimates, not definitive salary expectations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salary-scope-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
